diff --git a/kernel/cgroup/cgroup.c b/kernel/cgroup/cgroup.c
index a4325ebc6979..c3094ecbe2d5 100644
--- a/kernel/cgroup/cgroup.c
+++ b/kernel/cgroup/cgroup.c
@@ -68,6 +68,8 @@
 /* let's not notify more than 100 times per second */
 #define CGROUP_FILE_NOTIFY_MIN_INTV	DIV_ROUND_UP(HZ, 100)
 
+void proc_cgroup_cache_clear(struct task_struct *tsk);
+
 /*
  * cgroup_mutex is the master lock.  Any modification to cgroup or its
  * hierarchy must be performed while holding it.
@@ -855,6 +857,12 @@ static void css_set_skip_task_iters(struct css_set *cset,
 		css_task_iter_skip(it, task);
 }
 
+#include <linux/livepatch.h>
+
+#define SHADOW_MUTEX	0
+#define SHADOW_CACHE	1
+#define SHADOW_KEY	2
+
 /**
  * css_set_move_task - move a task from one css_set to another
  * @task: task being moved
@@ -874,6 +882,7 @@ static void css_set_move_task(struct task_struct *task,
 			      struct css_set *from_cset, struct css_set *to_cset,
 			      bool use_mg_tasks)
 {
+	struct mutex *proc_cgroup_mutex = klp_shadow_get(task, SHADOW_MUTEX);
 	lockdep_assert_held(&css_set_lock);
 
 	if (to_cset && !css_set_populated(to_cset))
@@ -883,7 +892,10 @@ static void css_set_move_task(struct task_struct *task,
 		WARN_ON_ONCE(list_empty(&task->cg_list));
 
 		css_set_skip_task_iters(from_cset, task);
+		if (proc_cgroup_mutex) mutex_lock(proc_cgroup_mutex);
 		list_del_init(&task->cg_list);
+		if (proc_cgroup_mutex) proc_cgroup_cache_clear(task);
+		if (proc_cgroup_mutex) mutex_unlock(proc_cgroup_mutex);
 		if (!css_set_populated(from_cset))
 			css_set_update_populated(from_cset, false);
 	} else {
@@ -899,8 +911,11 @@ static void css_set_move_task(struct task_struct *task,
 		WARN_ON_ONCE(task->flags & PF_EXITING);
 
 		cgroup_move_task(task, to_cset);
+		if (proc_cgroup_mutex) mutex_lock(proc_cgroup_mutex);
 		list_add_tail(&task->cg_list, use_mg_tasks ? &to_cset->mg_tasks :
 							     &to_cset->tasks);
+		if (proc_cgroup_mutex) proc_cgroup_cache_clear(task);
+		if (proc_cgroup_mutex) mutex_unlock(proc_cgroup_mutex);
 	}
 }
 
@@ -2359,6 +2374,7 @@ static void cgroup_attach_unlock(bool lock_threadgroup)
 static void cgroup_migrate_add_task(struct task_struct *task,
 				    struct cgroup_mgctx *mgctx)
 {
+	struct mutex *proc_cgroup_mutex = klp_shadow_get(task, SHADOW_MUTEX);
 	struct css_set *cset;
 
 	lockdep_assert_held(&css_set_lock);
@@ -2376,7 +2392,10 @@ static void cgroup_migrate_add_task(struct task_struct *task,
 
 	mgctx->tset.nr_tasks++;
 
+	if (proc_cgroup_mutex) mutex_lock(proc_cgroup_mutex);
 	list_move_tail(&task->cg_list, &cset->mg_tasks);
+	if (proc_cgroup_mutex) proc_cgroup_cache_clear(task);
+	if (proc_cgroup_mutex) mutex_unlock(proc_cgroup_mutex);
 	if (list_empty(&cset->mg_node))
 		list_add_tail(&cset->mg_node,
 			      &mgctx->tset.src_csets);
@@ -5955,6 +5974,67 @@ void cgroup_path_from_kernfs_id(u64 id, char *buf, size_t buflen)
 	kernfs_put(kn);
 }
 
+void proc_cgroup_cache_clear(struct task_struct *tsk)
+{
+	void **caches = klp_shadow_get(tsk, SHADOW_CACHE);
+	void **keys = klp_shadow_get(tsk, SHADOW_KEY);
+	int i;
+
+	if (!caches || !keys)
+		return;
+
+	for (i = 0; i < 16; i++) {
+		if (keys[i] != 0) {
+			keys[i] = 0;
+			if (caches[i])
+				kfree(caches[i]);
+		};
+	};
+}
+
+/* Needs tsk->proc_cgroup_mutex */
+char *proc_cgroup_cache_lookup(struct task_struct *tsk, struct cgroup_namespace *srchkey)
+{
+	void **caches = klp_shadow_get(tsk, SHADOW_CACHE);
+	void **keys = klp_shadow_get(tsk, SHADOW_KEY);
+	int i;
+
+	if (!caches || !keys)
+		return NULL;
+
+	for (i = 0; i < 16; i++) {
+		if (keys[i] == srchkey)
+			return caches[i];
+	};
+	return NULL;
+}
+
+/* Needs tsk->proc_cgroup_mutex */
+char *proc_cgroup_cache_alloc(struct task_struct *tsk, struct cgroup_namespace *srchkey, char* buf, size_t len)
+{
+	void **caches = klp_shadow_get(tsk, SHADOW_CACHE);
+	void **keys = klp_shadow_get(tsk, SHADOW_KEY);
+	int i;
+	char *ret;
+
+	if (!caches || !keys)
+		return NULL;
+
+	for (i = 0; i < 16; i++) {
+		if (!keys[i]) {
+			ret = kmalloc(len+1, GFP_KERNEL);
+			if (!ret)
+				return NULL;
+			caches[i] = ret;
+			keys[i] = srchkey;
+			memcpy(ret, buf, len);
+			return ret;
+		};
+	};
+	proc_cgroup_cache_clear(tsk);
+	return NULL;
+}
+
 /*
  * proc_cgroup_show()
  *  - Print task's cgroup paths into seq_file, one line for each hierarchy
@@ -5963,11 +6043,23 @@ void cgroup_path_from_kernfs_id(u64 id, char *buf, size_t buflen)
 int proc_cgroup_show(struct seq_file *m, struct pid_namespace *ns,
 		     struct pid *pid, struct task_struct *tsk)
 {
+	char *cache;
 	char *buf;
-	int retval;
+	int retval = -ENOMEM;
 	struct cgroup_root *root;
+	struct mutex *proc_cgroup_mutex = klp_shadow_get(tsk, SHADOW_MUTEX);
+
+	if (proc_cgroup_mutex) {
+		mutex_lock(proc_cgroup_mutex);
+		cache = proc_cgroup_cache_lookup(tsk, current->nsproxy->cgroup_ns);
+		if (cache) {
+			seq_puts(m, cache);
+			mutex_unlock(proc_cgroup_mutex);
+			return 0;
+		}
+		mutex_unlock(proc_cgroup_mutex);
+	}
 
-	retval = -ENOMEM;
 	buf = kmalloc(PATH_MAX, GFP_KERNEL);
 	if (!buf)
 		goto out;
@@ -6024,6 +6116,11 @@ int proc_cgroup_show(struct seq_file *m, struct pid_namespace *ns,
 			seq_putc(m, '\n');
 	}
 
+	if (proc_cgroup_mutex) {
+		mutex_lock(proc_cgroup_mutex);
+		cache = proc_cgroup_cache_alloc(tsk, current->nsproxy->cgroup_ns, m->buf, m->size);
+		mutex_unlock(proc_cgroup_mutex);
+	}
 	retval = 0;
 out_unlock:
 	spin_unlock_irq(&css_set_lock);
@@ -6338,13 +6435,17 @@ void cgroup_exit(struct task_struct *tsk)
 	struct cgroup_subsys *ss;
 	struct css_set *cset;
 	int i;
+	struct mutex *proc_cgroup_mutex = klp_shadow_get(tsk, SHADOW_MUTEX);
 
 	spin_lock_irq(&css_set_lock);
 
 	WARN_ON_ONCE(list_empty(&tsk->cg_list));
 	cset = task_css_set(tsk);
 	css_set_move_task(tsk, cset, NULL, false);
+	if (proc_cgroup_mutex) mutex_lock(proc_cgroup_mutex);
 	list_add_tail(&tsk->cg_list, &cset->dying_tasks);
+	if (proc_cgroup_mutex) proc_cgroup_cache_clear(tsk);
+	if (proc_cgroup_mutex) mutex_unlock(proc_cgroup_mutex);
 	cset->nr_tasks--;
 
 	WARN_ON_ONCE(cgroup_task_frozen(tsk));
@@ -6363,6 +6464,7 @@ void cgroup_release(struct task_struct *task)
 {
 	struct cgroup_subsys *ss;
 	int ssid;
+	struct mutex *proc_cgroup_mutex = klp_shadow_get(task, SHADOW_MUTEX);
 
 	do_each_subsys_mask(ss, ssid, have_release_callback) {
 		ss->release(task);
@@ -6370,7 +6472,10 @@ void cgroup_release(struct task_struct *task)
 
 	spin_lock_irq(&css_set_lock);
 	css_set_skip_task_iters(task_css_set(task), task);
+	if (proc_cgroup_mutex) mutex_lock(proc_cgroup_mutex);
 	list_del_init(&task->cg_list);
+	if (proc_cgroup_mutex) proc_cgroup_cache_clear(task);
+	if (proc_cgroup_mutex) mutex_unlock(proc_cgroup_mutex);
 	spin_unlock_irq(&css_set_lock);
 }
 
diff --git a/kernel/fork.c b/kernel/fork.c
index 6a060869f94c..03ecc99eab55 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -418,11 +418,41 @@ static int memcg_charge_kernel_stack(struct task_struct *tsk)
 	return 0;
 }
 
+#include <linux/livepatch.h>
+
+#define SHADOW_MUTEX	0
+#define SHADOW_CACHE	1
+#define SHADOW_KEY	2
+
+static int proc_cgroup_mutex_ctor(void *obj, void *shadow_data, void *ctor_data)
+{
+	struct mutex *mutex = (struct mutex *)shadow_data;
+	mutex_init(mutex);
+	return 0;
+}
+
+static int proc_cgroup_caches_ctor(void *obj, void *shadow_data, void *ctor_data)
+{
+	void *caches = shadow_data;
+	memset(caches, 0, sizeof(void *) * 16);
+	return 0;
+}
+
+static int proc_cgroup_keys_ctor(void *obj, void *shadow_data, void *ctor_data)
+{
+	void *keys = shadow_data;
+	memset(keys, 0, sizeof(void *) * 16);
+	return 0;
+}
+
 static void release_task_stack(struct task_struct *tsk)
 {
 	if (WARN_ON(tsk->state != TASK_DEAD))
 		return;  /* Better to leak the stack than to free prematurely */
 
+	klp_shadow_free(tsk, SHADOW_MUTEX, NULL);
+	klp_shadow_free(tsk, SHADOW_CACHE, NULL);
+	klp_shadow_free(tsk, SHADOW_KEY, NULL);
 	account_kernel_stack(tsk, -1);
 	free_thread_stack(tsk);
 	tsk->stack = NULL;
@@ -2077,6 +2107,14 @@ static __latent_entropy struct task_struct *copy_process(
 	p->sequential_io	= 0;
 	p->sequential_io_avg	= 0;
 #endif
+#ifdef CONFIG_CGROUPS
+	klp_shadow_alloc(p, SHADOW_MUTEX,
+	    sizeof(struct mutex), GFP_KERNEL, proc_cgroup_mutex_ctor, NULL);
+	klp_shadow_alloc(p, SHADOW_CACHE,
+	    sizeof(void *) * 16, GFP_KERNEL, proc_cgroup_caches_ctor, NULL);
+	klp_shadow_alloc(p, SHADOW_KEY,
+	    sizeof(void *) * 16, GFP_KERNEL, proc_cgroup_keys_ctor, NULL);
+#endif
 
 	/* Perform scheduler related setup. Assign this task to a CPU. */
 	retval = sched_fork(clone_flags, p);
diff --git a/kernel/vpsadminos.c b/kernel/vpsadminos.c
index 4be1c38461ee..18b7b165c444 100644
--- a/kernel/vpsadminos.c
+++ b/kernel/vpsadminos.c
@@ -9,6 +9,69 @@
 #include <linux/xarray.h>
 #include <asm/page.h>
 #include "sched/sched.h"
+#include <linux/vpsadminos-livepatch.h>
+#include "kpatch-macros.h"
+char old_uname[65];
+char new_uname[65];
+
+#define SHADOW_MUTEX	0
+#define SHADOW_CACHE	1
+#define SHADOW_KEY	2
+
+static int proc_cgroup_mutex_ctor(void *obj, void *shadow_data, void *ctor_data)
+{
+	struct mutex *mutex = (struct mutex *)shadow_data;
+	mutex_init(mutex);
+	return 0;
+}
+
+static int proc_cgroup_caches_ctor(void *obj, void *shadow_data, void *ctor_data)
+{
+	void *caches = shadow_data;
+	memset(caches, 0, sizeof(void *) * 16);
+	return 0;
+}
+
+static int proc_cgroup_keys_ctor(void *obj, void *shadow_data, void *ctor_data)
+{
+	void *keys = shadow_data;
+	memset(keys, 0, sizeof(void *) * 16);
+	return 0;
+}
+
+static int patch(patch_object *obj)
+{
+	struct task_struct *p;
+	rcu_read_lock();
+	for_each_process(p) {
+		klp_shadow_alloc(p, SHADOW_MUTEX,
+		    sizeof(struct mutex), GFP_KERNEL, proc_cgroup_mutex_ctor, NULL);
+		klp_shadow_alloc(p, SHADOW_CACHE,
+		    sizeof(void *) * 16, GFP_KERNEL, proc_cgroup_caches_ctor, NULL);
+		klp_shadow_alloc(p, SHADOW_KEY,
+		    sizeof(void *) * 16, GFP_KERNEL, proc_cgroup_keys_ctor, NULL);
+	}
+	rcu_read_unlock();
+	scnprintf(new_uname, 64, "%s.%s", LIVEPATCH_ORIG_KERNEL_VERSION,
+	    LIVEPATCH_NAME);
+	scnprintf(old_uname, 64, "%s", init_uts_ns.name.release);
+	scnprintf(init_uts_ns.name.release, 64, "%s", new_uname);
+	return 0;
+}
+KPATCH_PRE_PATCH_CALLBACK(patch);
+static void unpatch(patch_object *obj)
+{
+	struct task_struct *p;
+	rcu_read_lock();
+	for_each_process(p) {
+		klp_shadow_free_all(SHADOW_MUTEX, NULL);
+		klp_shadow_free_all(SHADOW_CACHE, NULL);
+		klp_shadow_free_all(SHADOW_KEY, NULL);
+	}
+	rcu_read_unlock();
+	scnprintf(init_uts_ns.name.release, 64, "%s", old_uname);
+}
+KPATCH_POST_UNPATCH_CALLBACK(unpatch);
 
 int online_cpus_in_cpu_cgroup(struct task_struct *p)
 {
diff --git a/mm/memcontrol.c b/mm/memcontrol.c
index f064f318a6a7..6b47defa88f4 100644
--- a/mm/memcontrol.c
+++ b/mm/memcontrol.c
@@ -85,6 +85,11 @@ static bool cgroup_memory_nosocket;
 /* Kernel memory accounting disabled? */
 static bool cgroup_memory_nokmem;
 
+bool cgroup_memory_kmem_enabled(void)
+{
+	return !cgroup_memory_nokmem;
+}
+
 int cgroup_memory_ksoftlimd_for_all = 0;
 int cgroup_memory_ksoftlimd_sleep_msec = 1000;
 int cgroup_memory_ksoftlimd_loops = 256;
diff --git a/mm/vmscan.c b/mm/vmscan.c
index 71e07964f948..7a56614c2374 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -2616,6 +2616,8 @@ static inline bool should_continue_reclaim(struct pglist_data *pgdat,
 	return inactive_lru_pages > pages_for_compaction;
 }
 
+extern bool cgroup_memory_kmem_enabled(void);
+
 static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)
 {
 	struct mem_cgroup *target_memcg = sc->target_mem_cgroup;
@@ -2662,8 +2664,9 @@ static void shrink_node_memcgs(pg_data_t *pgdat, struct scan_control *sc)
 
 		shrink_lruvec(lruvec, sc);
 
-		shrink_slab(sc->gfp_mask, pgdat->node_id, memcg,
-			    sc->priority);
+		if (current_is_kswapd() || cgroup_memory_kmem_enabled())
+			shrink_slab(sc->gfp_mask, pgdat->node_id, memcg,
+				    sc->priority);
 
 		/* Record the group's reclaim efficiency */
 		vmpressure(sc->gfp_mask, memcg, false,
